{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = RuntimeWarning) #ignore runtime warnings, which occur due to the presence of NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('adult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize features\n",
    "data.hist(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value counts for the categorical features: workclass, education, marital-status, occupation, relationship\n",
    "for feature in ['workclass', 'education', 'marital-status', 'occupation', 'relationship']:\n",
    "    print('------------ ' + feature + ' ---------------')\n",
    "    print(data[feature].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical and ordinal columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we encode education as categorical; there's not an obvious way to encode an ordinality for some college vs the two associate's degrees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cat = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'native-country', 'race', 'sex']\n",
    "\n",
    "features_cat = data[cols_cat]\n",
    "features_numerical = data.drop(columns = cols_cat + ['income']).convert_dtypes()\n",
    "\n",
    "outcome = data['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical values in the input features, we use one-hot encoding. We first check that we have no missing data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cat.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_array = ohe.fit_transform(features_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = ohe.get_feature_names_out()\n",
    "features_cat_onehot = pd.DataFrame(cat_array, columns=feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cat_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_cat_onehot.hist(figsize=(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_indicators = features_cat_onehot.columns[features_cat_onehot.columns.str.contains('nan')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indicator in nan_indicators: \n",
    "    feature = indicator.split('_nan')[0]\n",
    "    other_indicators = features_cat_onehot.columns[features_cat_onehot.columns.str.contains(feature)]\n",
    "    missing_mask = features_cat_onehot[indicator] == 1\n",
    "    features_cat_onehot.loc[missing_mask,other_indicators] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: verify - would imputation drop these indicators, or keep them in addition? \n",
    "for indicator in nan_indicators: \n",
    "    features_cat_onehot.drop(columns=indicator, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.concat([features_cat_onehot, outcome], axis=1)\n",
    "\n",
    "features = features_numerical.join([features_cat])\n",
    "df = pd.concat([features, outcome], axis=1)\n",
    "\n",
    "full_df = pd.concat([features_numerical, features_cat_onehot, outcome], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the code requires encoding the categorical columns (factors) with numerical levels. To ensure consistency, we save a list of all the levels of these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [(col, sorted(df[col][df[col].notnull()].unique())) for col in sorted(cols_cat)]\n",
    "with open(\"factor_levels.json\", \"w\", encoding=\"UTF-8\") as levelsfile:\n",
    "    json.dump(levels, levelsfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the imputation code requires knowing which columns are categorical and ordinal, so we store this information.  We now include yes/no (or similar) columns in the list of categorical columns.\n",
    "\n",
    "One of the imputation methods (MissForest) required encoding the one-hot columns as a single ordinal column; we also determine the column numbers of the categorical and ordinal columns for this encoded version.  For this purpose, we use a standalone variant of the `onehot_to_ord_multicat` method from `data_loader.py` that just returns the columns in the encoded version.  It does more than strictly needed, but that is to ensure it behaves as the `data_loader.py` method does.  Furthermore, the imputation methods only see the non-outcome columns, so we remove the outcome column before performing the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoders(factor_levels):\n",
    "    # factor_levels should be the `levels` variable created above\n",
    "    factors = [fl[0] for fl in factor_levels]\n",
    "    levels = [fl[1] for fl in factor_levels]\n",
    "\n",
    "    # sklearn requires us to fit a non-empty DataFrame even if we specify all\n",
    "    # the levels\n",
    "    dummy_df = pd.DataFrame({fl[0]: [fl[1][0]] for fl in factor_levels})\n",
    "    cat_colnames = factors\n",
    "    # building the model for transformations\n",
    "    ohe = OneHotEncoder(categories=levels, sparse=False)\n",
    "    onehot_encoder = ohe.fit(dummy_df)\n",
    "    encoded_colnames = ohe.get_feature_names_out(factors)\n",
    "    # building LabelEncoder dictionary model\n",
    "    orde = OrdinalEncoder(categories=levels)\n",
    "    ordinal_encoder = orde.fit(dummy_df)\n",
    "\n",
    "    return {\n",
    "        \"cat_colnames\": cat_colnames,\n",
    "        \"onehot_encoder\": onehot_encoder,\n",
    "        \"encoded_colnames\": encoded_colnames,\n",
    "        \"ordinal_encoder\": ordinal_encoder,\n",
    "    }\n",
    "\n",
    "\n",
    "def onehot_to_ord_columns(df, factor_levels):\n",
    "    encoders = get_encoders(factor_levels)\n",
    "    onehot_encoder = encoders[\"onehot_encoder\"]\n",
    "    ordinal_encoder = encoders[\"ordinal_encoder\"]\n",
    "    encoded_colnames = encoders[\"encoded_colnames\"]\n",
    "    cat_colnames = encoders[\"cat_colnames\"]\n",
    "\n",
    "    onehot_df = df[encoded_colnames]\n",
    "    oh_decoded = onehot_encoder.inverse_transform(onehot_df)\n",
    "    # silence warning in ordinal_encoder.transform\n",
    "    oh_decoded_df = pd.DataFrame(oh_decoded, columns=cat_colnames, index=df.index)\n",
    "    ord_df = ordinal_encoder.transform(oh_decoded_df)\n",
    "    ord_df = pd.DataFrame(ord_df, columns=cat_colnames, index=df.index)\n",
    "    rest_df = df.drop(encoded_colnames, axis=1)\n",
    "    converted_df = pd.concat([rest_df, ord_df], axis=1)\n",
    "    return list(converted_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_ord = []\n",
    "\n",
    "idxs = {}\n",
    "\n",
    "columns = list(df_onehot.columns)\n",
    "idx_cat = []\n",
    "for idx, col in enumerate(columns):\n",
    "    for cat in cols_cat:\n",
    "        if col.startswith(cat):\n",
    "            idx_cat.append(idx)\n",
    "idx_ord = [columns.index(col) for col in cols_ord]\n",
    "idxs[\"onehot\"] = [idx_cat, idx_ord]\n",
    "\n",
    "encoded_cols = onehot_to_ord_columns(df_onehot.dropna(), levels)#todo: more nuanced nan handling\n",
    "idx_cat = [encoded_cols.index(col) for col in cols_cat]\n",
    "idx_ord = [encoded_cols.index(col) for col in cols_ord]\n",
    "idxs[\"encoded\"] = [idx_cat, idx_ord]\n",
    "\n",
    "idxs[\"colnames\"] = {\"onehot\": columns, \"encoded\": encoded_cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"adult_cols.json\", \"w\", encoding=\"UTF-8\") as colsfile:\n",
    "    json.dump(idxs, colsfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also save the complete resulting dataset for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"complete_used.csv\", index=False)\n",
    "df_onehot.to_csv(\"complete_used_onehot.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv(f'./test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training, validation and holdout sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the one-hot encoded data to create the standard datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = Path('.')\n",
    "outdir.mkdir(exist_ok=True)\n",
    "\n",
    "n_splits = 10\n",
    "n_folds = 5\n",
    "idx = np.arange(len(df))\n",
    "\n",
    "kf_splits = KFold(n_splits=n_splits, random_state=1896, shuffle=True)\n",
    "\n",
    "for holdout_num, out_split in enumerate(kf_splits.split(idx)):\n",
    "    idx_train = idx[out_split[0]]\n",
    "    idx_test = idx[out_split[1]]\n",
    "    devel_fold = full_df.iloc[idx_train, ]\n",
    "    test_fold = full_df.iloc[idx_test, ]\n",
    "\n",
    "    test_fold.to_csv(outdir / f'holdout_{holdout_num}.csv', index=False)\n",
    "\n",
    "    kf_folds = KFold(n_splits=n_folds, random_state=165782 * holdout_num, shuffle=True)\n",
    "    idx_folds = np.arange(len(devel_fold))\n",
    "    for fold_num, idx_fold_split in enumerate(kf_folds.split(idx_folds)):\n",
    "        train_fold = devel_fold.iloc[idx_fold_split[0]]\n",
    "        val_fold = devel_fold.iloc[idx_fold_split[1]]\n",
    "        train_fold.to_csv(outdir / f'devel_{holdout_num}_train_{fold_num}.csv', index=False)\n",
    "        val_fold.to_csv(outdir / f'devel_{holdout_num}_val_{fold_num}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
