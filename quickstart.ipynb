{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages needed for Demo: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo uses the FastSparseGAMs package (https://github.com/interpretml/fastSparse), which needs to be installed for the demo to run. We can do in this notebook by uncommenting and running the below line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fastsparsegams==0.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we generate some simple synthetic data \n",
    "\n",
    "In this dataset, we'll have 5 covariates, and the outcome will simply be whether their sum is above the mean or not. However, we will also add informitive missingness to the first covariate, where missingness will only occur if the label is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd() + \"/missingness-experiments\")\n",
    "sys.path.append(os.getcwd() + \"/missingness-experiments/nature-mice-imputations\")\n",
    "from binarizer import Binarizer \n",
    "from visualization_utils import plot_shape_functions\n",
    "import fastsparsegams\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(0)\n",
    "n_rows = 1000\n",
    "n_cols = 5\n",
    "X = np.random.rand(n_rows, n_cols)\n",
    "y = 1 * (np.sum(X, axis=1) > np.mean(np.sum(X, axis=1)))\n",
    "\n",
    "# Add label-dependent missingness\n",
    "mask_out_x1 = (y == 1) & (np.random.rand(n_rows) > 0.5)\n",
    "X[mask_out_x1, 0] = np.nan\n",
    "\n",
    "# Convert to a pandas dataframe\n",
    "df = pd.DataFrame(X, columns=[f\"x{i}\" for i in range(n_cols)])\n",
    "df['label'] = y\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=20, stratify=df.values[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize and augment our input data\n",
    "\n",
    "Next, we binarize our input data and add missingness indicators and interaction terms. The binarization function below returns versions of the dataset without augmentation, with missingness indicators only, and with missingness indicators and interactions. We'll work with the final of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_quantiles = 3\n",
    "label = df.columns[-1]\n",
    "\n",
    "# Initialize our binarizer\n",
    "encoder = Binarizer(quantiles = np.linspace(0, 1, num_quantiles + 2)[1:-1], label=label, \n",
    "                    miss_vals=[np.nan], \n",
    "                    overall_mi_intercept = False, overall_mi_ixn = False, \n",
    "                    specific_mi_intercept = True, specific_mi_ixn = True, \n",
    "                    numerical_cols = df.columns[:-1], \n",
    "                    categorical_cols= [])\n",
    "\n",
    "# Generate our binarized and augmented data\n",
    "(\n",
    "    train_aug, \n",
    "    test_aug, \n",
    "    y_train_aug, \n",
    "    y_test_aug\n",
    ") = encoder.binarize_and_augment(\n",
    "    train_df, test_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, fit a model using fastsparsegams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using fastsparsegams, we can specify a grid of values for the L0 penalty;\n",
    "# each of these will produce a distinct model, so we'll need to specify which one we\n",
    "# want to work with later\n",
    "lambda_grid = [[20, 15, 10, 7.5, 5, 4, 3, 2, 1.5, 1, 0.5, 0.1]]\n",
    "model_aug = fastsparsegams.fit(\n",
    "    train_aug.astype(float), y_train_aug.astype(int)*2 - 1, loss=\"Exponential\", max_support_size=40, \n",
    "    algorithm=\"CDPSI\", lambda_grid=lambda_grid, num_lambda=None, num_gamma=None\n",
    ")\n",
    "\n",
    "selected_lambda = model_aug.lambda_0[0][-1]\n",
    "\n",
    "# Run our selected model over the train and test sets\n",
    "# This model outputs probabilities, so we threshold them to binary predictions\n",
    "preds = model_aug.predict(train_aug, selected_lambda) > 0.5\n",
    "print(f\"Train accuracy: {accuracy_score(preds, y_train_aug)}\")\n",
    "\n",
    "preds = model_aug.predict(test_aug, selected_lambda) > 0.5\n",
    "print(f\"Test accuracy: {accuracy_score(preds, y_test_aug)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shape_functions(\n",
    "    train_df,\n",
    "    encoder,\n",
    "    model_aug, \n",
    "    selected_lambda,\n",
    "    f'./gam_visualizations/tester_2.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m-gam-repl",
   "language": "python",
   "name": "m-gam-repl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
