{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastsparsegams\n",
    "np.random.seed(0)\n",
    "df = pd.read_csv('./fico_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=20, stratify=df['PoorRiskPerformance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_according_to_train(train_df, test_df):\n",
    "    n_train, d_train = train_df.shape\n",
    "    n_test, d_test = test_df.shape\n",
    "    train_binned, train_augmented_binned, test_binned, test_augmented_binned = {}, {}, {}, {}\n",
    "    train_no_missing, test_no_missing = {}, {}\n",
    "    bin_map = {}\n",
    "    dataset_structure_map = {}\n",
    "    thresh_vals = []\n",
    "    cur_new_col_index = 0\n",
    "    missing_inds = []\n",
    "    for col_ind, c in enumerate(train_df.columns):\n",
    "        quantiles = train_df[c][train_df[c] > -7].quantile([0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1]).unique()\n",
    "        bin_list = list(quantiles) + [-7, -8, -9]\n",
    "        dataset_structure_map[c] = {}\n",
    "        dataset_structure_map[c]['intercepts'] = {}\n",
    "        dataset_structure_map[c]['bins'] = []\n",
    "\n",
    "        if c == 'PoorRiskPerformance':\n",
    "            continue\n",
    "        for bin_ind, v in enumerate(bin_list):\n",
    "            thresh_vals.append(v)\n",
    "            bin_map[bin_ind] = col_ind\n",
    "            if v in [-7, -8, -9]:\n",
    "                dataset_structure_map[c]['intercepts'][v] = cur_new_col_index\n",
    "                missing_inds.append(v)\n",
    "                new_col_name = f'{c}_{v}'\n",
    "\n",
    "                new_row_train = np.zeros(n_train)\n",
    "                new_row_train[train_df[c] == v] = 1\n",
    "                train_binned[new_col_name] = new_row_train\n",
    "                train_augmented_binned[new_col_name] = new_row_train\n",
    "                \n",
    "                new_row_test = np.zeros(n_test)\n",
    "                new_row_test[test_df[c] == v] = 1\n",
    "                test_binned[new_col_name] = new_row_test\n",
    "                test_augmented_binned[new_col_name] = new_row_test\n",
    "            else:\n",
    "                dataset_structure_map[c]['bins'] = dataset_structure_map[c]['bins'] + [cur_new_col_index]\n",
    "                new_col_name = f'{c} <= {v}'\n",
    "\n",
    "                new_row_train = np.zeros(n_train)\n",
    "                new_row_train[train_df[c] <= v] = 1\n",
    "                new_row_train[train_df[c] <= -5] = 0\n",
    "                \n",
    "                train_no_missing[new_col_name] = new_row_train\n",
    "                train_binned[new_col_name] = new_row_train\n",
    "                train_augmented_binned[new_col_name] = new_row_train\n",
    "                \n",
    "                new_row_test = np.zeros(n_test)\n",
    "                new_row_test[test_df[c] <= v] = 1\n",
    "                new_row_test[test_df[c] <= -5] = 0\n",
    "                test_no_missing[new_col_name] = new_row_test\n",
    "                test_binned[new_col_name] = new_row_test\n",
    "                test_augmented_binned[new_col_name] = new_row_test\n",
    "            cur_new_col_index += 1\n",
    "    \n",
    "    for c_outer in train_df.columns:\n",
    "        dataset_structure_map[c_outer]['interactions'] = {}\n",
    "        if c_outer == 'PoorRiskPerformance':\n",
    "            continue\n",
    "        for m_val in [-7, -8, -9]:\n",
    "            for c_inner in train_df.columns:\n",
    "                if c_inner == c_outer:\n",
    "                    continue \n",
    "                \n",
    "                if 'interactions' not in dataset_structure_map[c_inner]:\n",
    "                    dataset_structure_map[c_inner]['interactions'] = {}\n",
    "                if c_outer not in dataset_structure_map[c_inner]['interactions']:\n",
    "                    dataset_structure_map[c_inner]['interactions'][c_outer] = {}\n",
    "                if m_val not in dataset_structure_map[c_inner]['interactions'][c_outer]:\n",
    "                    dataset_structure_map[c_inner]['interactions'][c_outer][m_val] = []\n",
    "\n",
    "                quantiles = train_df[c_inner][train_df[c_inner] > -7].quantile([0.2, 0.4, 0.6, 0.8, 1]).unique()\n",
    "                for v in quantiles:\n",
    "                    if (v in [-7, -8, -9]) or c_inner == 'PoorRiskPerformance':\n",
    "                        continue\n",
    "                    else:\n",
    "                        thresh_vals.append(v)\n",
    "                        dataset_structure_map[c_inner]['interactions'][c_outer][m_val] = dataset_structure_map[c_inner]['interactions'][c_outer][m_val] + [cur_new_col_index]\n",
    "                        new_col_name = f'{c_outer}_missing_{m_val} & {c_inner} <= {v}'\n",
    "\n",
    "                        new_row_train = np.zeros(n_train)\n",
    "                        new_row_train[(train_df[c_outer] == m_val) & (train_df[c_inner] <= v)] = 1\n",
    "                        new_row_train[(train_df[c_outer] == m_val) & (train_df[c_inner] <= -5)] = 0\n",
    "                        train_augmented_binned[new_col_name] = new_row_train\n",
    "\n",
    "                        new_row_test = np.zeros(n_test)\n",
    "                        new_row_test[(test_df[c_outer] == m_val) & (test_df[c_inner] <= v)] = 1\n",
    "                        new_row_test[(test_df[c_outer] == m_val) & (test_df[c_inner] <= -5)] = 0\n",
    "                        test_augmented_binned[new_col_name] = new_row_test\n",
    "                        cur_new_col_index += 1\n",
    "    train_binned['PoorRiskPerformance'] = train_df['PoorRiskPerformance']\n",
    "    test_binned['PoorRiskPerformance'] = test_df['PoorRiskPerformance']\n",
    "    train_no_missing['PoorRiskPerformance'] = train_df['PoorRiskPerformance']\n",
    "    test_no_missing['PoorRiskPerformance'] = test_df['PoorRiskPerformance']\n",
    "    train_augmented_binned['PoorRiskPerformance'] = train_df['PoorRiskPerformance']\n",
    "    test_augmented_binned['PoorRiskPerformance'] = test_df['PoorRiskPerformance']\n",
    "    return pd.DataFrame(train_no_missing), pd.DataFrame(train_binned), pd.DataFrame(train_augmented_binned), \\\n",
    "         pd.DataFrame(test_no_missing), pd.DataFrame(test_binned), pd.DataFrame(test_augmented_binned),\\\n",
    "        bin_map, missing_inds, dataset_structure_map, thresh_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_missing, train_binned, train_binned_augmented, test_no_missing, test_binned, test_binned_augmented, bin_map, missing_inds, dataset_structure_map, thresh_vals = binarize_according_to_train(train_df, test_df)\n",
    "\n",
    "X_indicator_train = train_binned[train_binned.columns[:-1]].values\n",
    "y_train = train_binned['PoorRiskPerformance'].values\n",
    "\n",
    "X_indicator_test = test_binned[test_binned.columns[:-1]].values\n",
    "y_test = test_binned['PoorRiskPerformance'].values\n",
    "\n",
    "X_no_missing_train = train_no_missing[train_no_missing.columns[:-1]].values\n",
    "X_no_missing_test = test_no_missing[test_no_missing.columns[:-1]].values\n",
    "\n",
    "X_aug_train = train_binned_augmented[train_binned_augmented.columns[:-1]].values\n",
    "X_aug_test = test_binned_augmented[test_binned_augmented.columns[:-1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepFunction:\n",
    "    def __init__(self, x_list=[1, 2], y_list=[1, 2]):\n",
    "        self.x_list = x_list\n",
    "        self.y_list = y_list\n",
    "\n",
    "    def add_cut(self, new_x, new_y):\n",
    "        if new_x in self.x_list:\n",
    "            x_arr = np.array(self.x_list)\n",
    "            y_arr = np.array(self.y_list)\n",
    "\n",
    "            y_arr[x_arr >= new_x] += new_y\n",
    "\n",
    "            self.y_list = list(y_arr)\n",
    "        else:\n",
    "            x_argsort = np.array(self.x_list + [new_x]).argsort()\n",
    "            x_arr = np.array(self.x_list + [new_x])[x_argsort]\n",
    "\n",
    "            if np.array(self.y_list)[np.array(self.x_list) < new_x].shape[0] > 0:\n",
    "                new_y_adjusted = np.array(self.y_list)[np.array(self.x_list) < new_x][-1] + new_y\n",
    "            else:\n",
    "                new_y_adjusted = new_y\n",
    "            y_arr = np.array(self.y_list + [new_y_adjusted])[x_argsort]\n",
    "            y_arr[x_arr > new_x] += new_y\n",
    "\n",
    "            self.x_list = list(np.array(self.x_list + [new_x])[x_argsort])\n",
    "            self.y_list = list(y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curve(dataset_structure_map, thresh_vals, coefs,\n",
    "             target_var='PercentTradesWBalance', missing_vars=(['ExternalRiskEstimate'], [-7]),\n",
    "             inters=True):\n",
    "    sf = StepFunction([], [])\n",
    "    if target_var in missing_vars[0]:\n",
    "        target_type_ind = missing_vars[0].index(target_var)\n",
    "        target_type = missing_vars[1][target_type_ind]\n",
    "\n",
    "        ind_of_interest = dataset_structure_map[target_var]['intercepts'][target_type]\n",
    "        return StepFunction([-1], [coefs[ind_of_interest]])\n",
    "\n",
    "            \n",
    "    sf = StepFunction([], [])\n",
    "    for b in dataset_structure_map[target_var]['bins']:\n",
    "        sf.add_cut(thresh_vals[b], coefs[b])\n",
    "    \n",
    "    if inters:\n",
    "        for other_var in dataset_structure_map[target_var]['interactions']:\n",
    "            if other_var in missing_vars[0]:\n",
    "                target_type_ind = missing_vars[0].index(other_var)\n",
    "                target_type = missing_vars[1][target_type_ind]\n",
    "\n",
    "                for b in dataset_structure_map[target_var]['interactions'][other_var][target_type]:\n",
    "                    sf.add_cut(thresh_vals[b], coefs[b])\n",
    "\n",
    "    return sf\n",
    "\n",
    "coefs = np.zeros(len(thresh_vals))\n",
    "coefs[2] = 3\n",
    "coefs[454] = 2\n",
    "sf = get_curve(dataset_structure_map, thresh_vals, coefs, target_var='PercentTradesWBalance', missing_vars=(['ExternalRiskEstimate'], [-7]))\n",
    "print(sf.x_list, sf.y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_indicator = LogisticRegression(max_iter=1000).fit(X_aug_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = model_indicator.coef_[0]\n",
    "(coefs != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_grid = [[20, 15, 10, 7.5, 5, 4, 3, 2, 1.5, 1, 0.5, 0.1]]\n",
    "model_indicator = fastsparsegams.fit(\n",
    "    X_indicator_train.astype(float), y_train.astype(int)*2 - 1, loss=\"Exponential\", \n",
    "    max_support_size=40, algorithm=\"CDPSI\", lambda_grid=lambda_grid, num_lambda=None, num_gamma=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aug= fastsparsegams.fit(\n",
    "    X_aug_train.astype(float), y_train.astype(int)*2 - 1, loss=\"Exponential\", max_support_size=40, \n",
    "    algorithm=\"CDPSI\", lambda_grid=lambda_grid, num_lambda=None, num_gamma=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_interesting_missingness(dataset_structure_map, coefs, inters=True):\n",
    "    miss_cols = []\n",
    "    miss_types = []\n",
    "    for v in dataset_structure_map:\n",
    "        if inters:\n",
    "            for v_inter in dataset_structure_map[v]['interactions']:\n",
    "                cur = dataset_structure_map[v]['interactions'][v_inter]\n",
    "                for m_type in cur:\n",
    "                    for val in cur[m_type]:\n",
    "                        if coefs[val] != 0:\n",
    "                            miss_cols.append(v_inter)\n",
    "                            miss_types.append(m_type)\n",
    "        for m_type in dataset_structure_map[v]['intercepts']:\n",
    "            coef = dataset_structure_map[v]['intercepts'][m_type]\n",
    "            if coefs[coef] != 0:\n",
    "                miss_cols.append(v)\n",
    "                miss_types.append(m_type)\n",
    "    return miss_cols, miss_types\n",
    "\n",
    "check_interesting_missingness(dataset_structure_map, coefs, inters=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inters = True\n",
    "if inters:\n",
    "    coefs = model_aug.coeff(lambda_0=model_aug.lambda_0[0][6]).toarray().flatten()\n",
    "else:\n",
    "    coefs = model_indicator.coeff(lambda_0=model_indicator.lambda_0[0][3]).toarray().flatten() #first entry is intercept\n",
    "\n",
    "\n",
    "inter = coefs[0]\n",
    "coefs = coefs[1:]\n",
    "\n",
    "possible_missing_vars = check_interesting_missingness(dataset_structure_map, coefs, inters=True)\n",
    "print(possible_missing_vars)\n",
    "print(len(possible_missing_vars[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "missingness_sets = [p for p in powerset(zip(possible_missing_vars[0], possible_missing_vars[1]))]\n",
    "missingness_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingness_sets = [p for p in powerset(zip(possible_missing_vars[0], possible_missing_vars[1]))]\n",
    "shaped_missingness_sets = []\n",
    "for s in missingness_sets:\n",
    "    names = [cur[0] for cur in s]\n",
    "    vals = [cur[1] for cur in s]\n",
    "    shaped_missingness_sets.append((names, vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import os\n",
    "os.makedirs('./gam_visualizations', exist_ok=True)\n",
    "sns.set(font_scale=2.0)\n",
    "sns.set_style(style='white')\n",
    "inters = False\n",
    "n_miss = 1\n",
    "missing_vals = (possible_missing_vars[0][:n_miss], possible_missing_vars[1][:n_miss])\n",
    "print(missing_vals)\n",
    "n_interesting = 0\n",
    "possible_active_vars = []\n",
    "inter_vars = []\n",
    "max_y = 0\n",
    "min_y = 0\n",
    "\n",
    "\n",
    "for var in dataset_structure_map:\n",
    "    c = get_curve(dataset_structure_map, thresh_vals, coefs, target_var=var, missing_vars=missing_vals, inters=inters)\n",
    "    if np.any(abs(np.array(c.y_list)) > 0):\n",
    "        n_interesting += 1\n",
    "        max_y = max(max_y, max(c.y_list))\n",
    "        min_y = min(min_y, min(c.y_list))\n",
    "\n",
    "    for m_set in shaped_missingness_sets:\n",
    "        c = get_curve(dataset_structure_map, thresh_vals, coefs, target_var=var, missing_vars=m_set, inters=inters)\n",
    "        if np.any(abs(np.array(c.y_list)) > 0):\n",
    "            max_y = max(max_y, max(c.y_list))\n",
    "            min_y = min(min_y, min(c.y_list))\n",
    "\n",
    "        if np.any(abs(np.array(c.y_list)) > 0) and (var not in possible_active_vars):\n",
    "            possible_active_vars.append(var)\n",
    "\n",
    "fig, ax = plt.subplots(1, len(possible_active_vars), figsize=(len(possible_active_vars)*4, 5.5))\n",
    "cur_i = 0\n",
    "for loop_iter in range(2):\n",
    "    for var in dataset_structure_map:\n",
    "        if (loop_iter==0) and (var not in missing_vals[0]):\n",
    "            continue\n",
    "        elif (loop_iter==1) and (var in missing_vals[0]):\n",
    "            continue\n",
    "\n",
    "        c = get_curve(dataset_structure_map, thresh_vals, coefs, target_var=var, missing_vars=missing_vals, inters=inters)\n",
    "        if np.any(abs(np.array(c.y_list)) > 0):\n",
    "            ax[cur_i].step([train_df[train_df[var]>-5][var].min()] + c.x_list + [train_df[train_df[var]>-5][var].max()],\n",
    "                        [c.y_list[0]] + c.y_list + [c.y_list[-1]], where='post')\n",
    "            ax[cur_i].set_title(var)\n",
    "            ax[cur_i].set_ylim(min_y-0.1, 0.1+max_y)\n",
    "            if cur_i > 0:\n",
    "                ax[cur_i].set_yticks([])\n",
    "            cur_i += 1\n",
    "        elif var in possible_active_vars:\n",
    "            ax[cur_i].step([train_df[train_df[var]>-5][var].min()] + [train_df[train_df[var]>-5][var].max()], [0] + [0], where='post')\n",
    "            ax[cur_i].set_title(var)\n",
    "            ax[cur_i].set_ylim(min_y-0.1, 0.1+max_y)\n",
    "            if cur_i > 0:\n",
    "                ax[cur_i].set_yticks([])\n",
    "            cur_i += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "fig.tight_layout(pad=4.0)\n",
    "plt.savefig(f'./gam_visualizations/tester.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97523d6d1cde124460ec5b2267a1ee9da83b236293ee79f3c0c0016a03f0dab9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 ('missing_data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
